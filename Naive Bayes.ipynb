{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Train = pd.read_csv('trainData.txt', sep=\" \", header=None)\n",
    "data_Label   = pd.read_csv('trainLabel.txt', sep=\" \", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = pd.read_csv('words.txt', sep=\" \", header=None).values\n",
    "#word = word.to_dict(orient='records')\n",
    "word_dict = {}\n",
    "count = 2\n",
    "for i in word:\n",
    "    myDict = {count : i[0]}\n",
    "    word_dict.update(myDict)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Train.columns = [\"docid\", \"wordid\"]\n",
    "data_Train=data_Train.groupby('docid')['wordid'].apply(list)\n",
    "\n",
    "for i in range(0,1499):\n",
    "    try:\n",
    "        data_Train[i]\n",
    "    except:\n",
    "        data_Train[i] = np.nan\n",
    "\n",
    "df = pd.DataFrame(columns=[\"Class\",\"words\"])\n",
    "\n",
    "for i in range(0,1500):\n",
    "    df = df.append({\"Class\":data_Label[0][i] ,\"words\" :data_Train[i+1]}, ignore_index=True)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "myList = [0] * 6969\n",
    "new_df = []\n",
    "for i in df.values:\n",
    "    myList[0] = i[0]\n",
    "    for j in i[1]:\n",
    "        myList[j+1] = 1\n",
    "    new_df.append(myList)\n",
    "    myList = [0] * 6969\n",
    "\n",
    "df = pd.DataFrame.from_records(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Train = pd.read_csv('testData.txt', sep=\" \", header=None)\n",
    "data_Label   = pd.read_csv('testLabel.txt', sep=\" \", header=None)\n",
    "\n",
    "data_Train.columns = [\"docid\", \"wordid\"]\n",
    "data_Train=data_Train.groupby('docid')['wordid'].apply(list)\n",
    "\n",
    "for i in range(0,1499):\n",
    "    try:\n",
    "        data_Train[i]\n",
    "    except:\n",
    "        data_Train[i] = np.nan\n",
    "\n",
    "test = pd.DataFrame(columns=[\"Class\",\"words\"])\n",
    "\n",
    "for i in range(0,1500):\n",
    "    test = test.append({\"Class\":data_Label[0][i] ,\"words\" :data_Train[i+1]}, ignore_index=True)\n",
    "\n",
    "test = test.dropna()\n",
    "\n",
    "myList = [0] * 6969\n",
    "new_df = []\n",
    "for i in test.values:\n",
    "    myList[0] = i[0]\n",
    "    for j in i[1]:\n",
    "        myList[j+1] = 1\n",
    "    new_df.append(myList)\n",
    "    myList = [0] * 6969\n",
    "\n",
    "test = pd.DataFrame.from_records(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob2 = len(df[df[0] == 2])\n",
    "prob1 = len(df[df[0] == 1])\n",
    "\n",
    "df1 = df[df[0] == 1]\n",
    "df2 = df[df[0] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = {}\n",
    "differences = {}\n",
    "l1 = len(df1) + 2\n",
    "l2 = len(df2) + 2\n",
    "for i in range(1,6969):\n",
    "    prob1 = (sum(df1[i]) + 1) / l1\n",
    "    prob2 = (sum(df2[i]) + 1) / l2\n",
    "    myDic = {i : {1 : prob1, 2 : prob2}}\n",
    "    probabilities.update(myDic)\n",
    "    myDic = {i : abs(log(prob1) - log(prob2))}\n",
    "    differences.update(myDic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "christian  -> 3.5862511805929826\n",
      "religion  -> 3.5142776809678935\n",
      "atheism  -> 3.298569108141202\n",
      "books  -> 3.239860109348644\n",
      "christians  -> 3.2216080670050733\n",
      "library  -> 3.2161435827313274\n",
      "religious  -> 3.0937746954951884\n",
      "libraries  -> 3.0883102112214424\n",
      "novel  -> 3.0883102112214424\n",
      "beliefs  -> 2.9984645156908636\n"
     ]
    }
   ],
   "source": [
    "top10 = list({k: v for k, v in sorted(differences.items(), key=lambda item: item[1], reverse= True)})[:10]\n",
    "\n",
    "for i in top10:\n",
    "    print(word_dict[i], \" ->\", differences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    pred1 = 1\n",
    "    pred2 = 1\n",
    "    for i in range(1,6969):\n",
    "        if X[i] == 1:\n",
    "            pred1 = pred1 * probabilities[i][1]\n",
    "            pred2 = pred2 * probabilities[i][2]\n",
    "        else:\n",
    "            pred1 = pred1 * (1 - probabilities[i][1])\n",
    "            pred2 = pred2 * (1 - probabilities[i][2])\n",
    "    if pred1 >= pred2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy : 74.55172413793103\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "l = len(test)\n",
    "for i in range(1,l):\n",
    "    p = predict(test.iloc[i])\n",
    "    predictions.append(p)\n",
    "\n",
    "count = 0\n",
    "actual = list(test[0])\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == actual[i]:\n",
    "        count += 1\n",
    "        \n",
    "print(\"Testing Accuracy :\", count*100/len(actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 91.3013698630137\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "l = len(df)\n",
    "for i in range(1,l):\n",
    "    p = predict(df.iloc[i])\n",
    "    predictions.append(p)\n",
    "\n",
    "count = 0\n",
    "actual = list(df[0])\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == actual[i]:\n",
    "        count += 1\n",
    "        \n",
    "print(\"Training Accuracy :\", count*100/len(actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
